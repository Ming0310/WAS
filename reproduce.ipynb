{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c41d87b",
   "metadata": {},
   "source": [
    "### Abstract\n",
    " Activation sparsity provides a dynamic, input-dependent alternative to weight pruning for accelerating inference in large language models (LLMs), effectively reducing unnecessary computations and memory accesses during the forward pass. Despite its promise, existing activation sparsification methods suffer from two major limitations: (1) solely relying on activation magnitude for sparsification, ignoring the coupling influence with the corresponding weights, (2) applying uniform sparsity rates across all blocks without considering block-wise sparsity sensitivity. To address these issues, this paper proposes a novel training-free weight-aware activation sparsity framework, called WAS. Firstly, with analyzing the coupling relationshape between weight and activation, we introduce a weight-aware scoring method to measure the activation importance in sparsification. Then, a novel constrained Bayesian optimization algorithm is further devised to set a suitable sparsity ratio for all blocks based on the sparsity sensitivity. Finally, we implement a custom GPU sparsity kernel to support the resulting sparsity patterns for wall-clock decoding speed-ups. Our WAS achieves competitive performance at 60\\% model-level sparsity and significantly outperforms prior methods at higher sparsity levels, achieving up to 1.68× inference speed-up—at no retraining or weight update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47539c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/teal_test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "import argparse\n",
    "import typing\n",
    "from utils.utils import get_tokenizer, get_sparse_model\n",
    "from eval_test.evaluate import eval_tasks\n",
    "\n",
    "from was.model import LlamaSparseForCausalLM, LlamaSparseConfig\n",
    "from was.model import MistralSparseForCausalLM, MistralSparseConfig\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "AutoConfig.register(\"llama_sparse\", LlamaSparseConfig)\n",
    "AutoConfig.register(\"mistral_sparse\", MistralSparseConfig)\n",
    "\n",
    "AutoModelForCausalLM.register(LlamaSparseConfig, LlamaSparseForCausalLM)\n",
    "AutoModelForCausalLM.register(MistralSparseConfig, MistralSparseForCausalLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa007cd",
   "metadata": {},
   "source": [
    "### Load the optimal sparsity rates obtained from Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645727e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = {\n",
    "    \"Mistral-7B\": {\n",
    "        'q': 1, 'k': 1/8, 'v': 1/8, 'o': 1,\n",
    "        'gate': 3.5, 'up': 3.5, 'down': 3.5\n",
    "    },\n",
    "\n",
    "        \"Llama-2-7B\": {\n",
    "        'q': 1, 'k': 1/8, 'v': 1/8, 'o': 1,\n",
    "        'gate': 2.6875, 'up': 2.6875, 'down': 2.6875\n",
    "        },\n",
    "\n",
    "\n",
    "}\n",
    "sps={\n",
    "\"Llama-2-7B\" : [[0.3720121707624662, 0.37498553980085, 0.376696045782326, 0.37769695534272746, 0.37846142740563493, 0.37882172794166535, 0.3798171617619194, 0.38074865530775154, 0.38109715204992267, 0.3820352978287211, 0.383533766815422, 0.38775520433175414, 0.3904868305517332, 0.3916333827297726, 0.3919130484524919, 0.3920800042447037, 0.3924149854253026, 0.39270114103356535, 0.3933065710110301, 0.3950363430939929, 0.3967854329668602, 0.4016094384892526, 0.40208250102195237, 0.40210378276978204, 0.40304778157101795, 0.40343212943818424, 0.4084919309351512, 0.41839918068408477, 0.4271652402472407, 0.42861908583054953, 0.42966470560633424, 0.42986860433344914],\n",
    "        [0.5713442230921849, 0.5729181462011875, 0.5751639345726678, 0.5765424467032879, 0.577812537008091, 0.5788743852102369, 0.580099415373599, 0.5811054410117681, 0.5817902779633589, 0.5824603532022575, 0.5831172735445226, 0.5847044104133572, 0.5865813829885432, 0.5907346920796416, 0.5932687962435536, 0.5943618828546953, 0.59522312014206, 0.5967951211449557, 0.5976843344014812, 0.5986977179977958, 0.5996316135565588, 0.6002875202634645, 0.6008302710337199, 0.6008843171192886, 0.6022971107009933, 0.6054288504626175, 0.6124941033486824, 0.6206212181934292, 0.6271763150339046, 0.6284894662250705, 0.6294855924705518, 0.6296899443219208],\n",
    "        [0.7012713644882108, 0.7014350676570817, 0.7015280287890517, 0.70180846964121, 0.7027857076118573, 0.7086458806420335, 0.7094407211949303, 0.7097986219585672, 0.7121104654860442, 0.7128657978637374, 0.7132423515600318, 0.7163301931877049, 0.7227644916204667, 0.7310197223644707, 0.7458138392890786, 0.7545503694559393, 0.7594123738446797, 0.7657121497330586, 0.7690828972998875, 0.7717372693123751, 0.7741929299961626, 0.7768396395880095, 0.7775849192360305, 0.7780837574048741, 0.7784494308440698, 0.7784499331817754, 0.7784501452774784, 0.7784524379257576, 0.7784596848464203, 0.7785002267222303, 0.7785018032213286, 0.7785059298425341]\n",
    "        ],\n",
    "\"Mistral-7B\" : [[0.3775609028955764, 0.3789939381523285, 0.3804538615950444, 0.3815614306996619, 0.38224594047171234, 0.3837599738379501, 0.3850329422840084, 0.386834105212761, 0.3870595402345686, 0.3876852650395319, 0.3881267457750328, 0.3889023527537393, 0.3900865502739806, 0.39175575848080446, 0.39224317793138114, 0.39225849220071995, 0.39254832551000446, 0.3926732644425294, 0.393007677847419, 0.39349594360508006, 0.3940331265477053, 0.394223388650777, 0.3955972728965453, 0.4000183496171237, 0.4030123777110685, 0.403901405721781, 0.4137566951141697, 0.4233744438906073, 0.4255043498401377, 0.42635197414865533, 0.4292518869062295, 0.4298838892980136],\n",
    "                    [0.5748498485047117, 0.5757952376670732, 0.5769846532769749, 0.5782101895963475, 0.5787955489200871, 0.579472568153386, 0.5799823774012058, 0.580419755292789, 0.580978258557723, 0.5819787511593875, 0.5829053709598148, 0.5835538956395441, 0.5847833999203588, 0.5848275320468758, 0.5853440810683013, 0.5854080832503255, 0.5868607378443262, 0.5869686115514824, 0.589263039481553, 0.5894255165579633, 0.589620505249984, 0.5914595898953209, 0.5987830078798729, 0.6084131095001821, 0.6179649528103596, 0.6266688657041428, 0.6290965076257066, 0.6297254074497288, 0.6298517403927066, 0.6298892550028785, 0.6299066985056049, 0.629929676390124],\n",
    "                    [0.7013955019836529, 0.701678849542781, 0.7034597867432563, 0.7037652257529714, 0.7095079310696334, 0.7097794738662017, 0.7097960409992665, 0.7104157390405182, 0.710444691488421, 0.711179273810033, 0.7128782501544767, 0.7159002291726712, 0.7240184027608946, 0.7338599653129236, 0.7483119353687419, 0.756557240370774, 0.7606095022810708, 0.7720511405223425, 0.7745868566643107, 0.77648281522203, 0.7774244157562832, 0.7782131110083039, 0.7786373733056609, 0.7789397259021511, 0.7791482410663874, 0.7791492910957347, 0.7791498764021699, 0.7791502718643817, 0.7792017164493609, 0.7793270648687193, 0.7793277404717623, 0.7793284377876534]\n",
    "]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d5a191",
   "metadata": {},
   "source": [
    "### Evaluate PPL and 0-shot tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84d006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type llama_sparse. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.52it/s]\n",
      "/dataset/ming/code/jupyter/utils/utils.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  histogram = torch.load(f\"{self.file_path}/histograms.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sparse PPL at sparsity level:  0.6\n",
      "========================================\n",
      "Evaluating greedy PPL\n",
      "get_wikitext2\n",
      "Evaluating ...\n",
      "wikitext2 Perplexity: 6.560258\n",
      "get_c4_new\n",
      "Evaluating ...\n",
      "c4_new Perplexity: 8.782909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12:16:07:16,756 WARNING  [huggingface.py:95] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2025-05-12:16:07:16,758 INFO     [huggingface.py:481] Using model type 'default'\n",
      "2025-05-12:16:07:16,774 WARNING  [huggingface.py:275] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2025-05-12:16:07:16,778 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-05-12:16:07:16,779 INFO     [evaluator.py:217] Using pre-initialized model\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011 (last modified on Fri Jun 14 16:08:58 2024) since it couldn't be found locally at piqa, or remotely on the Hugging Face Hub.\n",
      "2025-05-12:16:07:23,526 WARNING  [load.py:1394] Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011 (last modified on Fri Jun 14 16:08:58 2024) since it couldn't be found locally at piqa, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the dataset since allenai/ai2_arc couldn't be found on the Hugging Face Hub\n",
      "2025-05-12:16:07:37,679 WARNING  [load.py:1431] Using the latest cached version of the dataset since allenai/ai2_arc couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'ARC-Easy' at /root/.cache/huggingface/datasets/allenai___ai2_arc/ARC-Easy/0.0.0/210d026faf9955653af8916fad021475a3f00453 (last modified on Wed Oct 23 12:30:10 2024).\n",
      "2025-05-12:16:07:37,686 WARNING  [cache.py:94] Found the latest cached dataset configuration 'ARC-Easy' at /root/.cache/huggingface/datasets/allenai___ai2_arc/ARC-Easy/0.0.0/210d026faf9955653af8916fad021475a3f00453 (last modified on Wed Oct 23 12:30:10 2024).\n",
      "2025-05-12:16:07:57,444 WARNING  [evaluator.py:270] Overwriting default num_fewshot of winogrande from None to 0\n",
      "2025-05-12:16:07:57,447 WARNING  [evaluator.py:270] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2025-05-12:16:07:57,448 WARNING  [evaluator.py:270] Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "2025-05-12:16:07:57,449 WARNING  [evaluator.py:270] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2025-05-12:16:07:57,450 WARNING  [evaluator.py:270] Overwriting default num_fewshot of piqa from None to 0\n",
      "2025-05-12:16:07:57,453 INFO     [task.py:415] Building contexts for winogrande on rank 0...\n",
      "100%|██████████| 1267/1267 [00:00<00:00, 104095.57it/s]\n",
      "2025-05-12:16:07:57,513 INFO     [task.py:415] Building contexts for hellaswag on rank 0...\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3510.11it/s]\n",
      "2025-05-12:16:08:01,431 INFO     [task.py:415] Building contexts for arc_challenge on rank 0...\n",
      "100%|██████████| 1172/1172 [00:00<00:00, 1566.17it/s]\n",
      "2025-05-12:16:08:02,248 INFO     [task.py:415] Building contexts for arc_easy on rank 0...\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1593.77it/s]\n",
      "2025-05-12:16:08:03,861 INFO     [task.py:415] Building contexts for piqa on rank 0...\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1517.02it/s]\n",
      "2025-05-12:16:08:05,136 INFO     [evaluator.py:489] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|          | 0/60566 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Determined largest batch size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests:  25%|██▍       | 14871/60566 [04:53<12:30, 60.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Determined largest batch size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60566/60566 [11:34<00:00, 87.23it/s] \n",
      "2025-05-12:16:20:10,227 WARNING  [huggingface.py:1353] Failed to get model SHA for LlamaSparseForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaFlashAttention2(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "          (sparse_fns): ModuleDict(\n",
      "            (q): SparsifyFn()\n",
      "            (k): SparsifyFn()\n",
      "            (v): SparsifyFn()\n",
      "            (o): SparsifyFn()\n",
      "          )\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "          (norm2): LayerNorm((11008,), eps=1e-05, elementwise_affine=False)\n",
      "          (sparse_fns): ModuleDict(\n",
      "            (gate): SparsifyFn()\n",
      "            (up): SparsifyFn()\n",
      "            (down): SparsifyFn()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ") at revision main. Error: Repo id must be a string, not <class 'was.model.LlamaSparseForCausalLM'>: 'LlamaSparseForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaFlashAttention2(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "          (sparse_fns): ModuleDict(\n",
      "            (q): SparsifyFn()\n",
      "            (k): SparsifyFn()\n",
      "            (v): SparsifyFn()\n",
      "            (o): SparsifyFn()\n",
      "          )\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "          (norm2): LayerNorm((11008,), eps=1e-05, elementwise_affine=False)\n",
      "          (sparse_fns): ModuleDict(\n",
      "            (gate): SparsifyFn()\n",
      "            (up): SparsifyFn()\n",
      "            (down): SparsifyFn()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    Tasks    |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|--------|---|-----:|---|-----:|\n",
      "|arc_challenge|      1|none  |     0|acc     |↑  |0.3959|±  |0.0143|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.4096|±  |0.0144|\n",
      "|arc_easy     |      1|none  |     0|acc     |↑  |0.7311|±  |0.0091|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.7062|±  |0.0093|\n",
      "|hellaswag    |      1|none  |     0|acc     |↑  |0.5442|±  |0.0050|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.7304|±  |0.0044|\n",
      "|piqa         |      1|none  |     0|acc     |↑  |0.7720|±  |0.0098|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.7688|±  |0.0098|\n",
      "|winogrande   |      1|none  |     0|acc     |↑  |0.6654|±  |0.0133|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = \"/path/to/llama2-7b\"\n",
    "net = model_path.split(\"/\")[-1]\n",
    "was_path = \"./models/Llama-2-7B\"\n",
    "model_type = \"Llama-2-7B\"\n",
    "greedy_flag = True\n",
    "sparsity = 0.6\n",
    "eval_ppl = True\n",
    "tasks = \"piqa,arc_easy,arc_challenge,hellaswag,winogrande\"\n",
    "batch_size = \"auto:4.0\"\n",
    "fewshot = 0\n",
    "seed = 2\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer(model_path)\n",
    "model = get_sparse_model(model_path, device=\"cpu\", histogram_path=os.path.join(was_path, \"histograms\"))\n",
    "\n",
    "print(\"Evaluating sparse PPL at sparsity level: \", sparsity)\n",
    "print(\"=\"*40)\n",
    "if greedy_flag:\n",
    "    print(\"Evaluating greedy PPL\")\n",
    "    greedy_path = os.path.join(was_path, \"lookup\")\n",
    "    \n",
    "    tmp = sps[model_type]\n",
    "    if sparsity == 0.4:\n",
    "        sp = tmp[0]\n",
    "    elif sparsity == 0.6:\n",
    "        sp = tmp[1]\n",
    "    else:\n",
    "        sp = tmp[2]\n",
    "    model.load_greedy_sparsities(greedy_path, sparsity, sparsities=sp)\n",
    "else:\n",
    "    print(\"Evaluating uniform PPL\")\n",
    "    model.set_uniform_sparsity(sparsity)\n",
    "\n",
    "eval_tasks(model, tokenizer, eval_ppl, model_path, tasks, fewshot, batch_size, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b54687",
   "metadata": {},
   "source": [
    "### Evaluate PPL and 5-shot tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8707f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type mistral_sparse. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]\n",
      "/dataset/ming/code/jupyter/utils/utils.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  histogram = torch.load(f\"{self.file_path}/histograms.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sparse PPL at sparsity level:  0.75\n",
      "========================================\n",
      "Evaluating greedy PPL\n",
      "get_wikitext2\n",
      "Evaluating ...\n",
      "wikitext2 Perplexity: 10.336608\n",
      "get_c4_new\n",
      "Evaluating ...\n",
      "c4_new Perplexity: 14.033845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13:19:40:42,743 WARNING  [huggingface.py:95] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2025-05-13:19:40:42,745 INFO     [huggingface.py:481] Using model type 'default'\n",
      "2025-05-13:19:40:42,756 WARNING  [huggingface.py:275] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2025-05-13:19:40:42,759 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-05-13:19:40:42,760 INFO     [evaluator.py:217] Using pre-initialized model\n",
      "Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub\n",
      "2025-05-13:19:40:56,986 WARNING  [load.py:1377] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'main' at /root/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Wed Jan 22 12:54:13 2025).\n",
      "2025-05-13:19:40:56,992 WARNING  [cache.py:94] Found the latest cached dataset configuration 'main' at /root/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Wed Jan 22 12:54:13 2025).\n",
      "2025-05-13:19:42:34,418 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_abstract_algebra from None to 5\n",
      "2025-05-13:19:42:34,420 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_computer_security from None to 5\n",
      "2025-05-13:19:42:34,421 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_conceptual_physics from None to 5\n",
      "2025-05-13:19:42:34,422 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_astronomy from None to 5\n",
      "2025-05-13:19:42:34,423 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 5\n",
      "2025-05-13:19:42:34,423 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_statistics from None to 5\n",
      "2025-05-13:19:42:34,424 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 5\n",
      "2025-05-13:19:42:34,425 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_mathematics from None to 5\n",
      "2025-05-13:19:42:34,426 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_machine_learning from None to 5\n",
      "2025-05-13:19:42:34,427 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_chemistry from None to 5\n",
      "2025-05-13:19:42:34,427 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_anatomy from None to 5\n",
      "2025-05-13:19:42:34,427 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 5\n",
      "2025-05-13:19:42:34,428 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_biology from None to 5\n",
      "2025-05-13:19:42:34,428 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_computer_science from None to 5\n",
      "2025-05-13:19:42:34,428 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_physics from None to 5\n",
      "2025-05-13:19:42:34,429 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_electrical_engineering from None to 5\n",
      "2025-05-13:19:42:34,429 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_physics from None to 5\n",
      "2025-05-13:19:42:34,429 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_biology from None to 5\n",
      "2025-05-13:19:42:34,429 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 5\n",
      "2025-05-13:19:42:34,430 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_business_ethics from None to 5\n",
      "2025-05-13:19:42:34,430 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_miscellaneous from None to 5\n",
      "2025-05-13:19:42:34,430 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_virology from None to 5\n",
      "2025-05-13:19:42:34,431 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_global_facts from None to 5\n",
      "2025-05-13:19:42:34,431 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_accounting from None to 5\n",
      "2025-05-13:19:42:34,431 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_medicine from None to 5\n",
      "2025-05-13:19:42:34,432 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_management from None to 5\n",
      "2025-05-13:19:42:34,432 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_college_medicine from None to 5\n",
      "2025-05-13:19:42:34,432 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_nutrition from None to 5\n",
      "2025-05-13:19:42:34,433 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_human_aging from None to 5\n",
      "2025-05-13:19:42:34,433 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 5\n",
      "2025-05-13:19:42:34,433 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_marketing from None to 5\n",
      "2025-05-13:19:42:34,434 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_medical_genetics from None to 5\n",
      "2025-05-13:19:42:34,434 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_psychology from None to 5\n",
      "2025-05-13:19:42:34,434 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_security_studies from None to 5\n",
      "2025-05-13:19:42:34,434 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_sociology from None to 5\n",
      "2025-05-13:19:42:34,435 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_human_sexuality from None to 5\n",
      "2025-05-13:19:42:34,435 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_psychology from None to 5\n",
      "2025-05-13:19:42:34,435 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_geography from None to 5\n",
      "2025-05-13:19:42:34,436 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_public_relations from None to 5\n",
      "2025-05-13:19:42:34,436 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 5\n",
      "2025-05-13:19:42:34,436 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_econometrics from None to 5\n",
      "2025-05-13:19:42:34,437 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 5\n",
      "2025-05-13:19:42:34,437 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 5\n",
      "2025-05-13:19:42:34,438 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 5\n",
      "2025-05-13:19:42:34,438 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_international_law from None to 5\n",
      "2025-05-13:19:42:34,438 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_philosophy from None to 5\n",
      "2025-05-13:19:42:34,439 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_world_religions from None to 5\n",
      "2025-05-13:19:42:34,439 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_logical_fallacies from None to 5\n",
      "2025-05-13:19:42:34,439 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_jurisprudence from None to 5\n",
      "2025-05-13:19:42:34,440 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_european_history from None to 5\n",
      "2025-05-13:19:42:34,440 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_professional_law from None to 5\n",
      "2025-05-13:19:42:34,440 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_formal_logic from None to 5\n",
      "2025-05-13:19:42:34,441 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_moral_scenarios from None to 5\n",
      "2025-05-13:19:42:34,441 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_us_history from None to 5\n",
      "2025-05-13:19:42:34,441 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_prehistory from None to 5\n",
      "2025-05-13:19:42:34,442 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_moral_disputes from None to 5\n",
      "2025-05-13:19:42:34,442 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_high_school_world_history from None to 5\n",
      "2025-05-13:19:42:34,442 WARNING  [evaluator.py:270] Overwriting default num_fewshot of gsm8k from 5 to 5\n",
      "2025-05-13:19:42:34,449 INFO     [task.py:415] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 160.26it/s]\n",
      "2025-05-13:19:42:35,093 INFO     [task.py:415] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 168.13it/s]\n",
      "2025-05-13:19:42:35,696 INFO     [task.py:415] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|██████████| 235/235 [00:01<00:00, 151.94it/s]\n",
      "2025-05-13:19:42:37,259 INFO     [task.py:415] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|██████████| 152/152 [00:01<00:00, 144.10it/s]\n",
      "2025-05-13:19:42:38,327 INFO     [task.py:415] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 145.02it/s]\n",
      "2025-05-13:19:42:39,024 INFO     [task.py:415] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|██████████| 216/216 [00:01<00:00, 155.21it/s]\n",
      "2025-05-13:19:42:40,429 INFO     [task.py:415] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|██████████| 270/270 [00:01<00:00, 164.06it/s]\n",
      "2025-05-13:19:42:42,091 INFO     [task.py:415] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 168.32it/s]\n",
      "2025-05-13:19:42:42,693 INFO     [task.py:415] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|██████████| 112/112 [00:00<00:00, 165.59it/s]\n",
      "2025-05-13:19:42:43,377 INFO     [task.py:415] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 166.64it/s]\n",
      "2025-05-13:19:42:43,985 INFO     [task.py:415] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|██████████| 135/135 [00:00<00:00, 169.56it/s]\n",
      "2025-05-13:19:42:44,789 INFO     [task.py:415] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|██████████| 378/378 [00:02<00:00, 166.68it/s]\n",
      "2025-05-13:19:42:47,077 INFO     [task.py:415] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|██████████| 144/144 [00:00<00:00, 167.17it/s]\n",
      "2025-05-13:19:42:47,949 INFO     [task.py:415] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 167.15it/s]\n",
      "2025-05-13:19:42:48,555 INFO     [task.py:415] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|██████████| 151/151 [00:00<00:00, 166.15it/s]\n",
      "2025-05-13:19:42:49,475 INFO     [task.py:415] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|██████████| 145/145 [00:00<00:00, 164.02it/s]\n",
      "2025-05-13:19:42:50,369 INFO     [task.py:415] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|██████████| 102/102 [00:00<00:00, 166.58it/s]\n",
      "2025-05-13:19:42:50,990 INFO     [task.py:415] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|██████████| 310/310 [00:01<00:00, 163.59it/s]\n",
      "2025-05-13:19:42:52,906 INFO     [task.py:415] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|██████████| 203/203 [00:01<00:00, 163.59it/s]\n",
      "2025-05-13:19:42:54,161 INFO     [task.py:415] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 166.39it/s]\n",
      "2025-05-13:19:42:54,770 INFO     [task.py:415] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|██████████| 783/783 [00:04<00:00, 166.99it/s]\n",
      "2025-05-13:19:42:59,497 INFO     [task.py:415] Building contexts for mmlu_virology on rank 0...\n",
      "100%|██████████| 166/166 [00:00<00:00, 167.26it/s]\n",
      "2025-05-13:19:43:00,503 INFO     [task.py:415] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 165.49it/s]\n",
      "2025-05-13:19:43:01,116 INFO     [task.py:415] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|██████████| 282/282 [00:01<00:00, 164.04it/s]\n",
      "2025-05-13:19:43:02,850 INFO     [task.py:415] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|██████████| 272/272 [00:01<00:00, 148.36it/s]\n",
      "2025-05-13:19:43:04,701 INFO     [task.py:415] Building contexts for mmlu_management on rank 0...\n",
      "100%|██████████| 103/103 [00:00<00:00, 152.71it/s]\n",
      "2025-05-13:19:43:05,382 INFO     [task.py:415] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|██████████| 173/173 [00:01<00:00, 150.55it/s]\n",
      "2025-05-13:19:43:06,542 INFO     [task.py:415] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|██████████| 306/306 [00:02<00:00, 148.73it/s]\n",
      "2025-05-13:19:43:08,617 INFO     [task.py:415] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|██████████| 223/223 [00:01<00:00, 158.57it/s]\n",
      "2025-05-13:19:43:10,039 INFO     [task.py:415] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|██████████| 265/265 [00:01<00:00, 164.74it/s]\n",
      "2025-05-13:19:43:11,665 INFO     [task.py:415] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|██████████| 234/234 [00:01<00:00, 162.78it/s]\n",
      "2025-05-13:19:43:13,115 INFO     [task.py:415] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 166.49it/s]\n",
      "2025-05-13:19:43:13,724 INFO     [task.py:415] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|██████████| 545/545 [00:03<00:00, 165.92it/s]\n",
      "2025-05-13:19:43:17,036 INFO     [task.py:415] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|██████████| 245/245 [00:01<00:00, 165.42it/s]\n",
      "2025-05-13:19:43:18,533 INFO     [task.py:415] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|██████████| 201/201 [00:01<00:00, 167.75it/s]\n",
      "2025-05-13:19:43:19,744 INFO     [task.py:415] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|██████████| 131/131 [00:00<00:00, 167.22it/s]\n",
      "2025-05-13:19:43:20,538 INFO     [task.py:415] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|██████████| 612/612 [00:03<00:00, 166.27it/s]\n",
      "2025-05-13:19:43:24,249 INFO     [task.py:415] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|██████████| 198/198 [00:01<00:00, 165.77it/s]\n",
      "2025-05-13:19:43:25,454 INFO     [task.py:415] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|██████████| 110/110 [00:00<00:00, 166.64it/s]\n",
      "2025-05-13:19:43:26,123 INFO     [task.py:415] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|██████████| 390/390 [00:02<00:00, 164.94it/s]\n",
      "2025-05-13:19:43:28,509 INFO     [task.py:415] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|██████████| 114/114 [00:00<00:00, 163.71it/s]\n",
      "2025-05-13:19:43:29,215 INFO     [task.py:415] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|██████████| 193/193 [00:01<00:00, 167.01it/s]\n",
      "2025-05-13:19:43:30,382 INFO     [task.py:415] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|██████████| 238/238 [00:01<00:00, 166.94it/s]\n",
      "2025-05-13:19:43:31,823 INFO     [task.py:415] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 165.90it/s]\n",
      "2025-05-13:19:43:32,435 INFO     [task.py:415] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|██████████| 121/121 [00:00<00:00, 164.77it/s]\n",
      "2025-05-13:19:43:33,178 INFO     [task.py:415] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|██████████| 311/311 [00:01<00:00, 164.68it/s]\n",
      "2025-05-13:19:43:35,084 INFO     [task.py:415] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|██████████| 171/171 [00:01<00:00, 162.67it/s]\n",
      "2025-05-13:19:43:36,147 INFO     [task.py:415] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|██████████| 163/163 [00:00<00:00, 165.99it/s]\n",
      "2025-05-13:19:43:37,139 INFO     [task.py:415] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|██████████| 108/108 [00:00<00:00, 167.91it/s]\n",
      "2025-05-13:19:43:37,790 INFO     [task.py:415] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|██████████| 165/165 [00:00<00:00, 166.25it/s]\n",
      "2025-05-13:19:43:38,794 INFO     [task.py:415] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|██████████| 1534/1534 [00:09<00:00, 164.87it/s]\n",
      "2025-05-13:19:43:48,171 INFO     [task.py:415] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|██████████| 126/126 [00:00<00:00, 164.94it/s]\n",
      "2025-05-13:19:43:48,946 INFO     [task.py:415] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|██████████| 895/895 [00:05<00:00, 158.77it/s]\n",
      "2025-05-13:19:43:54,628 INFO     [task.py:415] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|██████████| 204/204 [00:01<00:00, 163.94it/s]\n",
      "2025-05-13:19:43:55,885 INFO     [task.py:415] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|██████████| 324/324 [00:01<00:00, 166.51it/s]\n",
      "2025-05-13:19:43:57,850 INFO     [task.py:415] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|██████████| 346/346 [00:02<00:00, 164.90it/s]\n",
      "2025-05-13:19:43:59,969 INFO     [task.py:415] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|██████████| 237/237 [00:01<00:00, 168.45it/s]\n",
      "2025-05-13:19:44:01,392 INFO     [task.py:415] Building contexts for gsm8k on rank 0...\n",
      "100%|██████████| 1319/1319 [00:03<00:00, 334.86it/s]\n",
      "2025-05-13:19:44:05,361 INFO     [evaluator.py:489] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|          | 0/56168 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Determined largest batch size: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests:  25%|██▍       | 13953/56168 [15:29<21:17, 33.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:4.0. Detecting largest batch size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests:  25%|██▍       | 13984/56168 [15:41<21:16, 33.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determined largest batch size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests:  50%|████▉     | 27821/56168 [22:02<10:58, 43.06it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:4.0. Detecting largest batch size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests:  50%|████▉     | 27948/56168 [22:14<10:55, 43.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determined largest batch size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 56168/56168 [30:46<00:00, 30.42it/s]\n",
      "2025-05-13:20:17:11,437 INFO     [evaluator.py:489] Running generate_until requests\n",
      "Running generate_until requests:   0%|          | 0/1319 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto. Detecting largest batch size\n",
      "Determined Largest batch size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running generate_until requests: 100%|██████████| 1319/1319 [3:11:25<00:00,  8.71s/it]  \n",
      "2025-05-13:23:28:40,942 WARNING  [huggingface.py:1353] Failed to get model SHA for MistralSparseForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralFlashAttention2(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "          (sparse_fns): ModuleDict(\n",
      "            (q): SparsifyFn()\n",
      "            (k): SparsifyFn()\n",
      "            (v): SparsifyFn()\n",
      "            (o): SparsifyFn()\n",
      "          )\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "          (norm2): LayerNorm((14336,), eps=1e-05, elementwise_affine=False)\n",
      "          (sparse_fns): ModuleDict(\n",
      "            (gate): SparsifyFn()\n",
      "            (up): SparsifyFn()\n",
      "            (down): SparsifyFn()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ") at revision main. Error: Repo id must be a string, not <class 'was.model.MistralSparseForCausalLM'>: 'MistralSparseForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralFlashAttention2(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "          (sparse_fns): ModuleDict(\n",
      "            (q): SparsifyFn()\n",
      "            (k): SparsifyFn()\n",
      "            (v): SparsifyFn()\n",
      "            (o): SparsifyFn()\n",
      "          )\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "          (norm2): LayerNorm((14336,), eps=1e-05, elementwise_affine=False)\n",
      "          (sparse_fns): ModuleDict(\n",
      "            (gate): SparsifyFn()\n",
      "            (up): SparsifyFn()\n",
      "            (down): SparsifyFn()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                 Tasks                 |Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n",
      "|---------------------------------------|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n",
      "|gsm8k                                  |      3|flexible-extract|     5|exact_match|↑  |0.0417|±  |0.0055|\n",
      "|                                       |       |strict-match    |     5|exact_match|↑  |0.0402|±  |0.0054|\n",
      "|mmlu                                   |      2|none            |      |acc        |↑  |0.3934|±  |0.0040|\n",
      "| - humanities                          |      2|none            |      |acc        |↑  |0.3509|±  |0.0068|\n",
      "|  - formal_logic                       |      1|none            |     5|acc        |↑  |0.1905|±  |0.0351|\n",
      "|  - high_school_european_history       |      1|none            |     5|acc        |↑  |0.4000|±  |0.0383|\n",
      "|  - high_school_us_history             |      1|none            |     5|acc        |↑  |0.3873|±  |0.0342|\n",
      "|  - high_school_world_history          |      1|none            |     5|acc        |↑  |0.3966|±  |0.0318|\n",
      "|  - international_law                  |      1|none            |     5|acc        |↑  |0.5537|±  |0.0454|\n",
      "|  - jurisprudence                      |      1|none            |     5|acc        |↑  |0.5000|±  |0.0483|\n",
      "|  - logical_fallacies                  |      1|none            |     5|acc        |↑  |0.3620|±  |0.0378|\n",
      "|  - moral_disputes                     |      1|none            |     5|acc        |↑  |0.4104|±  |0.0265|\n",
      "|  - moral_scenarios                    |      1|none            |     5|acc        |↑  |0.2458|±  |0.0144|\n",
      "|  - philosophy                         |      1|none            |     5|acc        |↑  |0.4952|±  |0.0284|\n",
      "|  - prehistory                         |      1|none            |     5|acc        |↑  |0.4784|±  |0.0278|\n",
      "|  - professional_law                   |      1|none            |     5|acc        |↑  |0.2979|±  |0.0117|\n",
      "|  - world_religions                    |      1|none            |     5|acc        |↑  |0.4678|±  |0.0383|\n",
      "| - other                               |      2|none            |      |acc        |↑  |0.4442|±  |0.0088|\n",
      "|  - business_ethics                    |      1|none            |     5|acc        |↑  |0.4200|±  |0.0496|\n",
      "|  - clinical_knowledge                 |      1|none            |     5|acc        |↑  |0.4038|±  |0.0302|\n",
      "|  - college_medicine                   |      1|none            |     5|acc        |↑  |0.3699|±  |0.0368|\n",
      "|  - global_facts                       |      1|none            |     5|acc        |↑  |0.3300|±  |0.0473|\n",
      "|  - human_aging                        |      1|none            |     5|acc        |↑  |0.4619|±  |0.0335|\n",
      "|  - management                         |      1|none            |     5|acc        |↑  |0.5049|±  |0.0495|\n",
      "|  - marketing                          |      1|none            |     5|acc        |↑  |0.5983|±  |0.0321|\n",
      "|  - medical_genetics                   |      1|none            |     5|acc        |↑  |0.5000|±  |0.0503|\n",
      "|  - miscellaneous                      |      1|none            |     5|acc        |↑  |0.5402|±  |0.0178|\n",
      "|  - nutrition                          |      1|none            |     5|acc        |↑  |0.4248|±  |0.0283|\n",
      "|  - professional_accounting            |      1|none            |     5|acc        |↑  |0.2872|±  |0.0270|\n",
      "|  - professional_medicine              |      1|none            |     5|acc        |↑  |0.3603|±  |0.0292|\n",
      "|  - virology                           |      1|none            |     5|acc        |↑  |0.3434|±  |0.0370|\n",
      "| - social sciences                     |      2|none            |      |acc        |↑  |0.4449|±  |0.0089|\n",
      "|  - econometrics                       |      1|none            |     5|acc        |↑  |0.3158|±  |0.0437|\n",
      "|  - high_school_geography              |      1|none            |     5|acc        |↑  |0.4444|±  |0.0354|\n",
      "|  - high_school_government_and_politics|      1|none            |     5|acc        |↑  |0.5648|±  |0.0358|\n",
      "|  - high_school_macroeconomics         |      1|none            |     5|acc        |↑  |0.3923|±  |0.0248|\n",
      "|  - high_school_microeconomics         |      1|none            |     5|acc        |↑  |0.3950|±  |0.0318|\n",
      "|  - high_school_psychology             |      1|none            |     5|acc        |↑  |0.4862|±  |0.0214|\n",
      "|  - human_sexuality                    |      1|none            |     5|acc        |↑  |0.4275|±  |0.0434|\n",
      "|  - professional_psychology            |      1|none            |     5|acc        |↑  |0.4314|±  |0.0200|\n",
      "|  - public_relations                   |      1|none            |     5|acc        |↑  |0.5091|±  |0.0479|\n",
      "|  - security_studies                   |      1|none            |     5|acc        |↑  |0.3551|±  |0.0306|\n",
      "|  - sociology                          |      1|none            |     5|acc        |↑  |0.4925|±  |0.0354|\n",
      "|  - us_foreign_policy                  |      1|none            |     5|acc        |↑  |0.6200|±  |0.0488|\n",
      "| - stem                                |      2|none            |      |acc        |↑  |0.3565|±  |0.0084|\n",
      "|  - abstract_algebra                   |      1|none            |     5|acc        |↑  |0.3000|±  |0.0461|\n",
      "|  - anatomy                            |      1|none            |     5|acc        |↑  |0.4074|±  |0.0424|\n",
      "|  - astronomy                          |      1|none            |     5|acc        |↑  |0.4934|±  |0.0407|\n",
      "|  - college_biology                    |      1|none            |     5|acc        |↑  |0.4097|±  |0.0411|\n",
      "|  - college_chemistry                  |      1|none            |     5|acc        |↑  |0.3100|±  |0.0465|\n",
      "|  - college_computer_science           |      1|none            |     5|acc        |↑  |0.3300|±  |0.0473|\n",
      "|  - college_mathematics                |      1|none            |     5|acc        |↑  |0.2700|±  |0.0446|\n",
      "|  - college_physics                    |      1|none            |     5|acc        |↑  |0.2157|±  |0.0409|\n",
      "|  - computer_security                  |      1|none            |     5|acc        |↑  |0.5600|±  |0.0499|\n",
      "|  - conceptual_physics                 |      1|none            |     5|acc        |↑  |0.3617|±  |0.0314|\n",
      "|  - electrical_engineering             |      1|none            |     5|acc        |↑  |0.4000|±  |0.0408|\n",
      "|  - elementary_mathematics             |      1|none            |     5|acc        |↑  |0.2778|±  |0.0231|\n",
      "|  - high_school_biology                |      1|none            |     5|acc        |↑  |0.4774|±  |0.0284|\n",
      "|  - high_school_chemistry              |      1|none            |     5|acc        |↑  |0.3350|±  |0.0332|\n",
      "|  - high_school_computer_science       |      1|none            |     5|acc        |↑  |0.4000|±  |0.0492|\n",
      "|  - high_school_mathematics            |      1|none            |     5|acc        |↑  |0.2704|±  |0.0271|\n",
      "|  - high_school_physics                |      1|none            |     5|acc        |↑  |0.2980|±  |0.0373|\n",
      "|  - high_school_statistics             |      1|none            |     5|acc        |↑  |0.3981|±  |0.0334|\n",
      "|  - machine_learning                   |      1|none            |     5|acc        |↑  |0.2500|±  |0.0411|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/path/to/Mistral-7B-v0.1\"\n",
    "net = model_path.split(\"/\")[-1]\n",
    "was_path = \"./models/Mistral-7B\"\n",
    "model_type = \"Mistral-7B\"\n",
    "greedy_flag = True\n",
    "sparsity = 0.75\n",
    "eval_ppl = True\n",
    "tasks = \"gsm8k,mmlu\"\n",
    "batch_size = \"auto:4.0\"\n",
    "fewshot = 5\n",
    "seed = 2\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer(model_path)\n",
    "model = get_sparse_model(model_path, device=\"cpu\", histogram_path=os.path.join(was_path, \"histograms\"))\n",
    "\n",
    "print(\"Evaluating sparse PPL at sparsity level: \", sparsity)\n",
    "print(\"=\"*40)\n",
    "if greedy_flag:\n",
    "    print(\"Evaluating greedy PPL\")\n",
    "    greedy_path = os.path.join(was_path, \"lookup\")\n",
    "    \n",
    "    tmp = sps[model_type]\n",
    "    if sparsity == 0.4:\n",
    "        sp = tmp[0]\n",
    "    elif sparsity == 0.6:\n",
    "        sp = tmp[1]\n",
    "    else:\n",
    "        sp = tmp[2]\n",
    "    model.load_greedy_sparsities(greedy_path, sparsity, sparsities=sp)\n",
    "else:\n",
    "    print(\"Evaluating uniform PPL\")\n",
    "    model.set_uniform_sparsity(sparsity)\n",
    "\n",
    "eval_tasks(model, tokenizer, eval_ppl, model_path, tasks, fewshot, batch_size, seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teal_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
